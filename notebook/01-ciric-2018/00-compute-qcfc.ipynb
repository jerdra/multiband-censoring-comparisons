{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "computational-mumbai",
   "metadata": {},
   "source": [
    "# Ciric QC-FC Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-engineering",
   "metadata": {},
   "source": [
    "Here we calculate the Quality Control - Functional Connectivity relation as described in Ciric et al. 2018:\n",
    "\n",
    "$$QCFC_{i,j} \\sim FD \\times FC_{i,j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coastal-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "import scipy.stats as scistats\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(\n",
    "    nb_workers=12,\n",
    "    progress_bar=True,\n",
    "    use_memory_fs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = \"/scratch/jjeyachandra/multiband-censoring-comparisons/\"\n",
    "subject_csvs = [\n",
    "    f for f in\n",
    "    os.listdir(os.path.join(scratch, \"output\"))\n",
    "    if \"sub-\" in f and f.endswith(\"tsv\")\n",
    "]\n",
    "f_meanfd = f\"{scratch}/output/meanFD.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "modified-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_REGEX = re.compile(\"sub-[A-Za-z0-9]+\")\n",
    "SES_REGEX = re.compile(\"ses-[A-Za-z0-9]+\")\n",
    "TASK_REGEX = re.compile(\"task-[A-Za-z0-9]+\")\n",
    "RUN_REGEX = re.compile(\"run-[A-Za-z0-9]+\")\n",
    "DESC_REGEX = re.compile(\"desc-[A-Za-z0-9]+\")\n",
    "MATCH_ENTITIES = [\n",
    "    'sub','ses','task','run'\n",
    "    ]\n",
    "\n",
    "def extract_csv(csv, entities):\n",
    "    '''\n",
    "    - Read in a subject connectivity CSV file\n",
    "    - Pull upper-triangular\n",
    "    - Remove ??? parcels\n",
    "    - Set entities\n",
    "    - Set row/col to category data-type\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "    os.path.join(scratch,\"output\", csv),\n",
    "    sep=\"\\t\",\n",
    "    index_col=0\n",
    "    )\n",
    "    \n",
    "    # Pivot to long format\n",
    "    df = df.where(np.triu(np.ones(df.shape)).astype(np.bool))\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = [\"row\",\"column\",\"value\"]\n",
    "    # Remove ??? columns\n",
    "    remove_mask = df[\"row\"].str.contains(\"\\?\\?\\?\") | \\\n",
    "              df[\"column\"].str.contains(\"\\?\\?\\?\")\n",
    "    df = df[~remove_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Set filename information\n",
    "    for k, v in entities:\n",
    "        df[k] = v\n",
    "    \n",
    "    # Convert row/column to categorical to save memory\n",
    "    df['row'] = df['row'].astype('category')\n",
    "    df['column'] = df['column'].astype('category')\n",
    "    return df\n",
    "\n",
    "def remap_dfs2long(subject_list, outdir):\n",
    "    '''\n",
    "    Convert connectivity matrices to long form and:\n",
    "    - add BIDS entity information to table\n",
    "    - convert row/column to categorical\n",
    "    - save in output directory as parquet/feather\n",
    "    '''\n",
    "    \n",
    "    result_list = []\n",
    "    for csv in subject_list:\n",
    "        sub_entities = (\n",
    "        (\"sub\", SUB_REGEX.search(csv)[0]),\n",
    "        (\"ses\", SES_REGEX.search(csv)[0]),\n",
    "        (\"task\",TASK_REGEX.search(csv)[0]),\n",
    "        (\"run\",RUN_REGEX.search(csv)[0]),\n",
    "        (\"desc\",DESC_REGEX.search(csv)[0])\n",
    "    )\n",
    "        out_file = \"_\".join([v for _,v in sub_entities])\n",
    "        out_csv = f\"{outdir}/{out_file}_connectivity.parquet\"\n",
    "        result_list.append(out_csv)\n",
    "        if os.path.exists(out_csv):\n",
    "            continue\n",
    "        df = extract_csv(csv, sub_entities)\n",
    "        df.to_parquet(out_csv,\n",
    "                 index=False)\n",
    "    return result_list \n",
    "\n",
    "def construct_df(parquets):\n",
    "    '''\n",
    "    - Read parquet file\n",
    "    - Convert entities into category type\n",
    "    '''\n",
    "    df = pd.concat([pd.read_parquet(s) for s in parquets])\n",
    "    df[['sub','ses','task','run','desc']] = \\\n",
    "        df[['sub','ses','task','run','desc']].astype('category')\n",
    "    return df\n",
    "\n",
    "def split_entities(row):\n",
    "    '''\n",
    "    Given row x, construct entities\n",
    "    '''\n",
    "    \n",
    "    def get_reg(reg, x):\n",
    "        res = reg.search(x)\n",
    "        return res[0] if res else np.nan\n",
    "    \n",
    "    x = row.entity\n",
    "    sub_entities = (\n",
    "        (\"sub\", get_reg(SUB_REGEX, x)),\n",
    "        (\"ses\", get_reg(SES_REGEX, x)),\n",
    "        (\"task\",get_reg(TASK_REGEX, x)),\n",
    "        (\"run\",get_reg(RUN_REGEX, x)),\n",
    "        (\"desc\",get_reg(DESC_REGEX, x))\n",
    "    )\n",
    "    \n",
    "    for k,v in sub_entities:\n",
    "        row[k] = v \n",
    "    return row\n",
    "\n",
    "\n",
    "def compute_qcfcs(df_list, fd, filters=None):\n",
    "    '''\n",
    "    Compute QC-FC statistics for each method\n",
    "    - Filter out by entities dictionary if given\n",
    "    - Group by method\n",
    "    - For each method append FDs\n",
    "    - For each edge compute pearson correlation\n",
    "    - Collect results\n",
    "    - Concatenate dataframes\n",
    "    \n",
    "    Uses pandarallel to parallelize across edge groups\n",
    "    to speed up computation significantly\n",
    "    '''\n",
    "    \n",
    "    FILTERS = {\n",
    "    'ses': 'ses-01',\n",
    "    'task': 'task-rest',\n",
    "    'run': 'run-1'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if not filters:\n",
    "        filters = FILTERS\n",
    "        \n",
    "    # Group DFs by method\n",
    "    df_by_method = groupby(df_list,\n",
    "                   key=lambda x: DESC_REGEX.search(x)[0])\n",
    "    \n",
    "    results = [] \n",
    "    for m, dfs in df_by_method:\n",
    "        print(f\"Processing method: {m}\")\n",
    "        df = construct_df(list(dfs))\n",
    "        \n",
    "        df_masks = np.logical_and.reduce(np.array([\n",
    "            df[k] == v for k,v in filters.items()\n",
    "        ]), axis=0)\n",
    "        df = df.iloc[df_masks, :]\n",
    "        \n",
    "        fd_masks = np.logical_and.reduce(np.array([\n",
    "            fd[k] == v for k,v in filters.items()\n",
    "        ]), axis=0)\n",
    "        fd = fd.iloc[fd_masks, :]\n",
    "        \n",
    "        fcqc = df.merge(fd, how='left',\n",
    "                       left_on=MATCH_ENTITIES,\n",
    "                       right_on=MATCH_ENTITIES)\n",
    "        \n",
    "        res = pd.DataFrame(\n",
    "                list(\n",
    "                    fcqc.groupby(['row','column'])\\\n",
    "                      .parallel_apply(qcfc))\n",
    "        )\n",
    "        res['method'] = m\n",
    "        results.append(res)\n",
    "        \n",
    "    return pd.concat(results)\n",
    "      \n",
    "def qcfc(g):\n",
    "    '''\n",
    "    Compute pearson correlation\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    row = g['row']\n",
    "    row = row[row.first_valid_index()]\n",
    "    col = g['column']\n",
    "    col = col[col.first_valid_index()]\n",
    "    \n",
    "    results = {\n",
    "        'source': row,\n",
    "        'target': col,\n",
    "        'pearson': np.nan,\n",
    "        'pvalue': np.nan,\n",
    "    }    \n",
    "    \n",
    "    if row == col:\n",
    "        return results\n",
    "    \n",
    "    r, p = scistats.pearsonr(g['mean_fd'], g['value'])\n",
    "    results.update({\n",
    "        'pearson': r,\n",
    "        'pvalue': p\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "explicit-palestinian",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_list = remap_dfs2long(subject_csvs, \"../../data/intermediate/\")\n",
    "df_list = sorted(df_list, key=lambda x: DESC_REGEX.search(x)[0])\n",
    "\n",
    "fd = pd.read_csv(f_meanfd)\\\n",
    "        .apply(split_entities, axis=1)\\\n",
    "        .dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "suitable-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing method: desc-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6725b67f0444eb8c60dc302048939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2930), Label(value='0 / 2930'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing method: desc-dct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5193ab4677ff4e2ca8d433bc2831de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2930), Label(value='0 / 2930'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing method: desc-fourier\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e6411fb1e846e89ec5c8d8f45f9535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2930), Label(value='0 / 2930'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing method: desc-powers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec5b90bcd214cbdb5b96f9d48e11c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2930), Label(value='0 / 2930'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = compute_qcfcs(df_list, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "sufficient-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = os.path.join(scratch,'output','QCFC_results.parquet')\n",
    "res.to_parquet(res_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
